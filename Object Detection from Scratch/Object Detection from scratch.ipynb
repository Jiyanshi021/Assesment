{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"joshideepak08\"\n",
        "os.environ[\"KAGGLE_API_KEY\"] = \"KGAT_ad5043e2d749348bd7ecb872480b8f19\"\n",
        "\n",
        "print(\"Username:\", os.environ.get(\"KAGGLE_USERNAME\"))\n",
        "print(\"Key exists:\", os.environ.get(\"KAGGLE_API_KEY\") is not None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iexrSnNiWBe",
        "outputId": "e0158d21-275e-4f2c-ca5e-a7c20529cfda"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Username: joshideepak08\n",
            "Key exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"joshideepak08\",\"key\":\"KGAT_ad5043e2d749348bd7ecb872480b8f19\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "qBFrJ6b8kWeJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"sakshamjn/vehicle-detection-8-classes-object-detection\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLpNxtZ8nU9M",
        "outputId": "32026a92-0e75-4c26-a453-ab9a75ccf416"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/sakshamjn/vehicle-detection-8-classes-object-detection?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 346M/346M [00:03<00:00, 111MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/sakshamjn/vehicle-detection-8-classes-object-detection/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/vehicle_dataset\n",
        "!cp -r /root/.cache/kagglehub/datasets/sakshamjn/vehicle-detection-8-classes-object-detection/versions/1/* data/vehicle_dataset\n"
      ],
      "metadata": {
        "id": "GZc0_BtTowMa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data/vehicle_dataset/train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukEII-IfpjbP",
        "outputId": "8fd1ea42-3201-4a15-c72a-ef988fde74b0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "images\tlabels\tlabels.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find data/vehicle_dataset/train -type f | head -n 20\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcSYl87zp9Ge",
        "outputId": "336bca2c-bc4e-4f93-9eb7-61e26c5a36cd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/vehicle_dataset/train/images/Highway_545_2020-07-30_jpg.rf.aac4ce2b6f8a145375f202b79c66a792.jpg\n",
            "data/vehicle_dataset/train/images/highway_3359_2020-08-26.jpg\n",
            "data/vehicle_dataset/train/images/ulu2450_jpg.rf.261c58216d131a33b9a3e379a940bcde.jpg\n",
            "data/vehicle_dataset/train/images/Highway_1236_2020-07-30_jpg.rf.ffca5dabfa87a2928053be64d92c6e59.jpg\n",
            "data/vehicle_dataset/train/images/ulu1534_jpg.rf.ed32678a013a30f516624b8be4b9bca4.jpg\n",
            "data/vehicle_dataset/train/images/highway_3583_2020-08-26.jpg\n",
            "data/vehicle_dataset/train/images/Highway_853_2020-07-30_jpg.rf.b56c050ec0cb16f7443a1fe21500efd5.jpg\n",
            "data/vehicle_dataset/train/images/Highway_1981_2020-07-30_jpg.rf.70cfad3a7189480cd5d3eb4412a6c979.jpg\n",
            "data/vehicle_dataset/train/images/Highway_1285_2020-07-30_jpg.rf.b79a7ab876471f7952f7654f66de6425.jpg\n",
            "data/vehicle_dataset/train/images/highway_3712_2020-08-26.jpg\n",
            "data/vehicle_dataset/train/images/Highway_869_2020-07-30_jpg.rf.7e13cb419157a87c2a2b9439328aeea8.jpg\n",
            "data/vehicle_dataset/train/images/Highway_593_2020-07-30_jpg.rf.c81e2610cfd8b5077eeb928369d76d22.jpg\n",
            "data/vehicle_dataset/train/images/Highway_1197_2020-07-30_jpg.rf.779e4363a5eaa174ecb9023ba828507c.jpg\n",
            "data/vehicle_dataset/train/images/highway_3457_2020-08-26.jpg\n",
            "data/vehicle_dataset/train/images/highway_2577_2020-08-26.jpg\n",
            "data/vehicle_dataset/train/images/ulu1494_jpg.rf.54782104112aa70147636bd4903f77e0.jpg\n",
            "data/vehicle_dataset/train/images/ulu1469_jpg.rf.748f4f9f59539498cd8c761cb2419f40.jpg\n",
            "data/vehicle_dataset/train/images/ulu71_jpg.rf.5e98b33f4539d16fde4ee754c8d66582.jpg\n",
            "data/vehicle_dataset/train/images/ulu1216_jpg.rf.c7d07aa61ab25ebdc165339235c79d9d.jpg\n",
            "data/vehicle_dataset/train/images/Highway_1431_2020-07-30_jpg.rf.fb35f180d3a0192a10836ea3cffc0250.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# -------- CONFIG --------\n",
        "DATA_ROOT = \"data/vehicle_dataset/train\"\n",
        "MAX_IMAGES = 2000        # speed + memory safe\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 2           # OOM-safe\n",
        "LR = 1e-3\n",
        "MAX_IMAGE_SIZE = 800     # resize large images\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsWYMgzHqdvw",
        "outputId": "b64aa3c3-74b0-4beb-e225-e56dd77eb42f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "CLASSES = {\n",
        "    0: \"car\",\n",
        "    1: \"bus\",\n",
        "    2: \"truck\",\n",
        "    3: \"motorcycle\",\n",
        "    4: \"bicycle\"\n",
        "}\n",
        "\n",
        "class VehicleDataset(Dataset):\n",
        "    def __init__(self, root_dir, classes, max_images=None):\n",
        "        self.image_dir = os.path.join(root_dir, \"images\")\n",
        "        self.label_dir = os.path.join(root_dir, \"labels\")\n",
        "        self.classes = classes\n",
        "\n",
        "        self.images = sorted(os.listdir(self.image_dir))\n",
        "        if max_images:\n",
        "            self.images = self.images[:max_images]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        h, w, _ = image.shape\n",
        "\n",
        "        # resize large images (VERY IMPORTANT)\n",
        "        if max(h, w) > MAX_IMAGE_SIZE:\n",
        "            scale = MAX_IMAGE_SIZE / max(h, w)\n",
        "            image = cv2.resize(image, (int(w*scale), int(h*scale)))\n",
        "            h, w, _ = image.shape\n",
        "\n",
        "        label_path = os.path.join(\n",
        "            self.label_dir, img_name.replace(\".jpg\", \".txt\")\n",
        "        )\n",
        "\n",
        "        boxes, labels = [], []\n",
        "\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path) as f:\n",
        "                for line in f:\n",
        "                    cls, xc, yc, bw, bh = map(float, line.split())\n",
        "                    if int(cls) not in self.classes:\n",
        "                        continue\n",
        "\n",
        "                    xmin = (xc - bw/2) * w\n",
        "                    ymin = (yc - bh/2) * h\n",
        "                    xmax = (xc + bw/2) * w\n",
        "                    ymax = (yc + bh/2) * h\n",
        "\n",
        "                    boxes.append([xmin, ymin, xmax, ymax])\n",
        "                    labels.append(int(cls) + 1)  # +1 background\n",
        "\n",
        "        if len(boxes) == 0:\n",
        "            return None\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "        image = torch.tensor(image/255., dtype=torch.float32).permute(2, 0, 1)\n",
        "        return image, target\n"
      ],
      "metadata": {
        "id": "t-LCUsGlp_Xn"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "dataset = VehicleDataset(\n",
        "    DATA_ROOT,\n",
        "    CLASSES,\n",
        "    max_images=MAX_IMAGES\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(\"Total training images:\", len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6amDooDyqVS4",
        "outputId": "c704ee70-8116-472e-f203-0ca879f7826f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training images: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CustomBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.out_channels = 256\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n"
      ],
      "metadata": {
        "id": "H8pq96WZqbF0"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "backbone = CustomBackbone()\n",
        "\n",
        "anchor_generator = AnchorGenerator(\n",
        "    sizes=((32, 64, 128, 256),),\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
        ")\n",
        "\n",
        "model = FasterRCNN(\n",
        "    backbone=backbone,\n",
        "    num_classes=len(CLASSES) + 1,\n",
        "    rpn_anchor_generator=anchor_generator,\n",
        "    rpn_pre_nms_top_n_train=600,\n",
        "    rpn_post_nms_top_n_train=300\n",
        ")\n",
        "\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YrFEkyWqmj5",
        "outputId": "ee9862b2-6a4a-44ba-f66f-557fc94bb4fd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): CustomBackbone(\n",
              "    (body): Sequential(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (5): ReLU()\n",
              "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (9): ReLU()\n",
              "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (12): ReLU()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=6, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "scaler = GradScaler()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsNJ6QHirAlE",
        "outputId": "a9bb0888-24de-4c22-e047-0b6d798d64ba"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-542241706.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        if batch is None:\n",
        "            continue\n",
        "\n",
        "        images, targets = batch\n",
        "\n",
        "        valid_images, valid_targets = [], []\n",
        "        for img, tgt in zip(images, targets):\n",
        "            if tgt[\"boxes\"].numel() == 0:\n",
        "                continue\n",
        "            valid_images.append(img)\n",
        "            valid_targets.append(tgt)\n",
        "\n",
        "        if len(valid_targets) == 0:\n",
        "            continue\n",
        "\n",
        "        images = [img.to(device) for img in valid_images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in valid_targets]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            loss_dict = model(images, targets)\n",
        "            loss = sum(loss_dict.values())\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        del loss_dict, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxawW_MgrEzt",
        "outputId": "952c6fe9-e724-4f02-fa0f-a2983a473adc"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipython-input-2744900887.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [04:37<00:00,  3.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3] | Loss: 799.1042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [04:13<00:00,  3.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/3] | Loss: 669.5180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [04:13<00:00,  3.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/3] | Loss: 687.7200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"faster_rcnn_vehicle_from_scratch.pth\")\n",
        "!ls -lh faster_rcnn_vehicle_from_scratch.pth\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-62YGHNfrI-k",
        "outputId": "0f674966-678a-408e-9a66-87335bf0f3e7"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 57M Jan 11 20:45 faster_rcnn_vehicle_from_scratch.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "img, _ = dataset[5]\n",
        "img = img.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model([img])\n",
        "\n",
        "print(out[0].keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4doYQECUzlyZ",
        "outputId": "71b00fc7-0e7f-42c8-a9fa-41110fedf663"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['boxes', 'labels', 'scores'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(box1, box2):\n",
        "    xA = max(box1[0], box2[0])\n",
        "    yA = max(box1[1], box2[1])\n",
        "    xB = min(box1[2], box2[2])\n",
        "    yB = min(box1[3], box2[3])\n",
        "\n",
        "    inter = max(0, xB - xA) * max(0, yB - yA)\n",
        "    area1 = (box1[2]-box1[0]) * (box1[3]-box1[1])\n",
        "    area2 = (box2[2]-box2[0]) * (box2[3]-box2[1])\n",
        "\n",
        "    union = area1 + area2 - inter\n",
        "    return inter / union if union > 0 else 0\n"
      ],
      "metadata": {
        "id": "Z0oaXRzSztiF"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_map(\n",
        "    model,\n",
        "    dataset,\n",
        "    iou_thresh=0.5,\n",
        "    score_thresh=0.5,\n",
        "    samples=200\n",
        "):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    checked = 0\n",
        "    idx = 0\n",
        "\n",
        "    while checked < samples and idx < len(dataset):\n",
        "        sample = dataset[idx]\n",
        "        idx += 1\n",
        "\n",
        "        # ðŸ”’ Skip invalid samples\n",
        "        if sample is None:\n",
        "            continue\n",
        "\n",
        "        img, target = sample\n",
        "        img = img.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model([img])[0]\n",
        "\n",
        "        gt_boxes = target[\"boxes\"]\n",
        "        pred_boxes = pred[\"boxes\"].cpu()\n",
        "        scores = pred[\"scores\"].cpu()\n",
        "\n",
        "        for pb, s in zip(pred_boxes, scores):\n",
        "            if s < score_thresh:\n",
        "                continue\n",
        "\n",
        "            total += 1\n",
        "            for gb in gt_boxes:\n",
        "                if compute_iou(pb, gb) >= iou_thresh:\n",
        "                    correct += 1\n",
        "                    break\n",
        "\n",
        "        checked += 1\n",
        "\n",
        "    return correct / total if total > 0 else 0\n"
      ],
      "metadata": {
        "id": "8XKEEQrM0PUq"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_50 = evaluate_map(model, dataset)\n",
        "print(\"Approx mAP@0.5:\", map_50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2n2iomm0QME",
        "outputId": "94836c70-a7db-4b3d-a615-e0f51fde2493"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approx mAP@0.5: 0.7692307692307693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# get one valid sample\n",
        "idx = 0\n",
        "while True:\n",
        "    sample = dataset[idx]\n",
        "    idx += 1\n",
        "    if sample is not None:\n",
        "        image, _ = sample\n",
        "        break\n",
        "\n",
        "image = image.to(device)\n",
        "\n",
        "# ---- GPU WARM-UP (VERY IMPORTANT) ----\n",
        "for _ in range(5):\n",
        "    with torch.no_grad():\n",
        "        _ = model([image])\n",
        "\n",
        "# ---- ACTUAL FPS MEASUREMENT ----\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    _ = model([image])\n",
        "end = time.time()\n",
        "\n",
        "fps = 1 / (end - start)\n",
        "\n",
        "print(f\"Inference Speed (FPS): {fps:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CByO2DTS0SNS",
        "outputId": "f1ace030-8b55-405d-c08c-e2e36c8d674a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Speed (FPS): 9.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh faster_rcnn_vehicle_from_scratch.pth\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzO15nUH1smR",
        "outputId": "834d21b8-59cb-42d9-fd8a-3ab298c37853"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 57M Jan 11 20:45 faster_rcnn_vehicle_from_scratch.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# ---------- VIDEO SETTINGS ----------\n",
        "OUTPUT_VIDEO = \"faster_rcnn_demo.mp4\"\n",
        "FPS = 10\n",
        "SCORE_THRESHOLD = 0.5\n",
        "MAX_FRAMES = 500   # short demo (10â€“20 sec)\n",
        "\n",
        "# ---------- GET IMAGE LIST ----------\n",
        "image_files = dataset.images[:MAX_FRAMES]\n",
        "\n",
        "# ---------- READ FIRST IMAGE FOR SIZE ----------\n",
        "first_img_path = os.path.join(DATA_ROOT, \"images\", image_files[0])\n",
        "frame = cv2.imread(first_img_path)\n",
        "h, w, _ = frame.shape\n",
        "\n",
        "# ---------- VIDEO WRITER ----------\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "video_writer = cv2.VideoWriter(\n",
        "    OUTPUT_VIDEO, fourcc, FPS, (w, h)\n",
        ")\n",
        "\n",
        "print(\"Creating demo video...\")\n",
        "\n",
        "# ---------- RUN INFERENCE FRAME BY FRAME ----------\n",
        "for img_name in tqdm(image_files):\n",
        "    img_path = os.path.join(DATA_ROOT, \"images\", img_name)\n",
        "\n",
        "    image = cv2.imread(img_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    img_tensor = torch.tensor(\n",
        "        image_rgb / 255.0,\n",
        "        dtype=torch.float32\n",
        "    ).permute(2, 0, 1).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model([img_tensor])[0]\n",
        "\n",
        "    boxes = output[\"boxes\"].cpu().numpy()\n",
        "    scores = output[\"scores\"].cpu().numpy()\n",
        "    labels = output[\"labels\"].cpu().numpy()\n",
        "\n",
        "    # ---------- DRAW BOXES ----------\n",
        "    for box, score, label in zip(boxes, scores, labels):\n",
        "        if score < SCORE_THRESHOLD:\n",
        "            continue\n",
        "\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "        text = f\"ID {label} | {score:.2f}\"\n",
        "        cv2.putText(\n",
        "            image, text,\n",
        "            (x1, max(y1 - 10, 10)),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.5, (0, 255, 0), 2\n",
        "        )\n",
        "\n",
        "    video_writer.write(image)\n",
        "\n",
        "video_writer.release()\n",
        "print(\"Video saved at:\", OUTPUT_VIDEO)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe9Cpt-J11NC",
        "outputId": "1e352882-e52c-49f9-f360-3095946a1722"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating demo video...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:41<00:00, 12.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video saved at: faster_rcnn_demo.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1TnQ0lVi2fA7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}